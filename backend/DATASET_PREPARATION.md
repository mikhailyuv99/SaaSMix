# Guide de PrÃ©paration du Dataset

## Structure des dossiers

**Organise tes fichiers comme Ã§a :**

```
dataset/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ vocal1.wav
â”‚   â”œâ”€â”€ vocal2.wav
â”‚   â”œâ”€â”€ vocal3.wav
â”‚   â””â”€â”€ ... (tous les vocals RAW)
â””â”€â”€ mixed/
    â”œâ”€â”€ vocal1.wav
    â”œâ”€â”€ vocal2.wav
    â”œâ”€â”€ vocal3.wav
    â””â”€â”€ ... (tous les vocals MIXÃ‰S - mÃªmes noms!)
```

**âš ï¸ IMPORTANT :**
- Les noms de fichiers doivent Ãªtre **identiques** dans `raw/` et `mixed/`
- Format recommandÃ© : **WAV** (44.1kHz, mono ou stÃ©rÃ©o)
- Chaque fichier dans `raw/` doit avoir son Ã©quivalent dans `mixed/`

---

## Conseils pour mixer les vocals

### 1. Utilise ton style de mixage habituel
- Mixe les vocals comme tu le ferais normalement
- Garde un style **cohÃ©rent** entre tous les fichiers
- C'est ce style que le modÃ¨le va apprendre !

### 2. QualitÃ© audio
- **Sample rate :** 44.1kHz (ou 48kHz, sera converti automatiquement)
- **Format :** WAV (non compressÃ©)
- **DurÃ©e :** 2-3 minutes minimum par fichier (plus c'est long, mieux c'est)

### 3. Normalisation
- Normalise les fichiers mixed Ã  **-0.3dB** maximum
- Ã‰vite le clipping
- Garde un niveau cohÃ©rent entre tous les fichiers

### 4. Nombre de fichiers
- **Minimum :** 50 paires
- **RecommandÃ© :** 100+ paires
- **IdÃ©al :** 200+ paires pour un modÃ¨le vraiment professionnel

---

## VÃ©rification avant entraÃ®nement

### Script de vÃ©rification (Ã  copier dans Colab) :

```python
import os
from pathlib import Path

raw_dir = "/content/dataset/raw"
mixed_dir = "/content/dataset/mixed"

# VÃ©rifier que les dossiers existent
if not os.path.exists(raw_dir):
    print(f"âŒ {raw_dir} n'existe pas!")
if not os.path.exists(mixed_dir):
    print(f"âŒ {mixed_dir} n'existe pas!")

# Lister les fichiers
raw_files = {f for f in os.listdir(raw_dir) if f.endswith(('.wav', '.mp3', '.flac'))}
mixed_files = {f for f in os.listdir(mixed_dir) if f.endswith(('.wav', '.mp3', '.flac'))}

# Trouver les paires
pairs = raw_files & mixed_files
missing_in_mixed = raw_files - mixed_files
missing_in_raw = mixed_files - raw_files

print(f"\nâœ… Paires trouvÃ©es: {len(pairs)}")
if missing_in_mixed:
    print(f"âš ï¸  {len(missing_in_mixed)} fichiers RAW sans Ã©quivalent MIXED:")
    for f in list(missing_in_mixed)[:5]:
        print(f"   - {f}")
if missing_in_raw:
    print(f"âš ï¸  {len(missing_in_raw)} fichiers MIXED sans Ã©quivalent RAW:")
    for f in list(missing_in_raw)[:5]:
        print(f"   - {f}")

# VÃ©rifier la durÃ©e totale
import librosa
total_duration = 0
for f in pairs:
    try:
        y, sr = librosa.load(os.path.join(raw_dir, f), sr=None)
        duration = len(y) / sr
        total_duration += duration
    except:
        pass

print(f"\nğŸ“Š DurÃ©e totale: {total_duration/60:.1f} minutes")
print(f"ğŸ“Š DurÃ©e moyenne par fichier: {total_duration/len(pairs)/60:.1f} minutes")

if len(pairs) >= 50:
    print("\nâœ… Dataset prÃªt pour l'entraÃ®nement!")
else:
    print(f"\nâš ï¸  Seulement {len(pairs)} paires. RecommandÃ©: 50+ paires")
```

---

## Quand tu es prÃªt Ã  entraÃ®ner

### 1. Compresse le dataset

```powershell
# Sur Windows, crÃ©e un ZIP du dossier dataset
# Envoie-le sur Google Drive
```

### 2. Dans Google Colab

**Cell 1 - Setup :**
```python
from google.colab import drive
drive.mount('/content/drive')
```

**Cell 2 - Upload dataset :**
```python
# Si tu as uploadÃ© dataset.zip sur Drive
import zipfile
import os

zip_path = "/content/drive/MyDrive/dataset.zip"
extract_path = "/content/"

if os.path.exists(zip_path):
    print("Extraction du dataset...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
    
    # VÃ©rifier la structure
    if os.path.exists("/content/dataset/dataset"):
        # Cas: dataset/dataset/raw et dataset/dataset/mixed
        os.system("mv /content/dataset/dataset/* /content/dataset/")
        os.rmdir("/content/dataset/dataset")
    
    print("âœ… Dataset extrait!")
else:
    print("âŒ dataset.zip non trouvÃ© dans Drive")
```

**Cell 3 - VÃ©rification :**
```python
# Copie le script de vÃ©rification ci-dessus
```

**Cell 4 - Upload Python files :**
```python
# Copie ml_model.py, data_loader.py, train_model.py depuis Drive
import shutil

files = ['ml_model.py', 'data_loader.py', 'train_model.py']
for f in files:
    src = f"/content/drive/MyDrive/{f}"
    dst = f"/content/{f}"
    if os.path.exists(src):
        shutil.copy(src, dst)
        print(f"âœ“ {f} copiÃ©")
    else:
        print(f"âœ— {f} introuvable")
```

**Cell 5 - Install dependencies :**
```python
!pip install torch torchaudio librosa soundfile tqdm tensorboard
```

**Cell 6 - Training (avec plus d'epochs) :**
```python
!python train_model.py \
    --raw_dir /content/dataset/raw \
    --mixed_dir /content/dataset/mixed \
    --output_dir /content/models \
    --batch_size 4 \
    --num_epochs 1000 \
    --learning_rate 0.00005 \
    --segment_length 176400
```

**Cell 7 - Download model :**
```python
# Ã€ la fin de l'entraÃ®nement
from google.colab import files
files.download('/content/models/best_model.pt')
```

---

## ParamÃ¨tres d'entraÃ®nement recommandÃ©s

### Pour 50-100 paires :
- **Epochs :** 500-1000
- **Batch size :** 4-8
- **Learning rate :** 0.00005
- **Temps estimÃ© :** 8-16 heures

### Pour 100-200+ paires :
- **Epochs :** 300-500
- **Batch size :** 8-16
- **Learning rate :** 0.0001
- **Temps estimÃ© :** 12-24 heures

---

## Checklist avant entraÃ®nement

- [ ] Au moins 50 paires raw/mixed
- [ ] Noms de fichiers identiques dans raw/ et mixed/
- [ ] Fichiers en format WAV (ou MP3/FLAC)
- [ ] DurÃ©e totale > 2 heures (idÃ©al)
- [ ] Style de mixage cohÃ©rent
- [ ] Pas de clipping dans les fichiers mixed
- [ ] Dataset uploadÃ© sur Google Drive
- [ ] Python files (ml_model.py, etc.) sur Drive

---

## Tips

1. **Mixe en batch :** Si tu as beaucoup de fichiers, crÃ©e un preset dans ton DAW et applique-le Ã  tous
2. **QualitÃ© > QuantitÃ© :** Mieux vaut 50 bons mixes que 100 mauvais
3. **CohÃ©rence :** Garde le mÃªme style de mixage pour tous les fichiers
4. **Backup :** Sauvegarde ton dataset avant de l'envoyer sur Drive

---

Bon courage pour la collecte de donnÃ©es ! ğŸµ
