# Adapter Diff-MST pour Vocal Mixing : Guide Pratique

## üéØ Objectif

Adapter Diff-MST (con√ßu pour multitrack) pour **vocal mixing single track** avec tes 222 paires.

---

## ‚ö†Ô∏è D√©fi Principal

**Diff-MST est con√ßu pour :**
- Plusieurs pistes (vocal, drums, bass, etc.)
- Reference mix (exemple de mix complet)
- Pr√©diction de param√®tres pour chaque piste

**On veut :**
- Single track (vocal seul)
- Raw vocal ‚Üí Mixed vocal (pas de reference mix)
- Apprendre ton style de mixing

---

## üîß Solution : Deux Approches

### Approche 1 : Utiliser Mixed comme "Reference Mix" (Plus Simple)

**Id√©e :** Traiter le mixed comme une "reference mix" et le raw comme la piste √† mixer.

**Comment :**
- Raw vocal = piste √† mixer
- Mixed vocal = reference mix (exemple de ce qu'on veut)
- Diff-MST apprend √† transformer raw ‚Üí mixed

**Avantages :**
- ‚úÖ Utilise l'architecture Diff-MST existante
- ‚úÖ Moins de modifications n√©cessaires
- ‚úÖ Plus simple √† impl√©menter

### Approche 2 : Adapter Compl√®tement pour Single Track (Plus Complexe)

**Id√©e :** Modifier Diff-MST pour travailler directement avec raw ‚Üí mixed.

**Comment :**
- Modifier le DataLoader
- Adapter le mod√®le pour single track
- Changer la loss function

**Avantages :**
- ‚úÖ Plus adapt√© √† notre cas
- ‚úÖ Plus efficace

**Inconv√©nients :**
- ‚ö†Ô∏è Plus de modifications n√©cessaires
- ‚ö†Ô∏è Plus complexe

---

## üöÄ Plan d'Action : Approche 1 (Recommand√©e)

### √âtape 1 : Installer Diff-MST dans Colab

```python
# Cell 1 : Setup
!git clone https://github.com/sai-soum/Diff-MST.git
%cd Diff-MST
!pip install -e .
!pip install torch torchaudio pytorch-lightning wandb librosa soundfile pyyaml tqdm

# Cell 2 : Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Cell 3 : Pr√©parer Dataset
!cp -r /content/drive/MyDrive/dataset /content/dataset
# Ou upload dataset.zip et d√©compresser
```

### √âtape 2 : Cr√©er DataLoader Custom

**Cr√©er `mst/dataloaders/vocal_reference_dataset.py` :**

```python
"""
DataLoader pour vocal mixing avec reference mix
Traite le mixed comme reference et le raw comme piste √† mixer
"""
import os
import torch
from torch.utils.data import Dataset
import librosa
import numpy as np
from typing import List, Tuple
import random

class VocalReferenceDataset(Dataset):
    """Dataset pour vocal mixing avec reference"""
    
    def __init__(
        self,
        raw_dir: str,
        mixed_dir: str,
        sample_rate: int = 48000,
        segment_length: int = 192000,  # 4 secondes
        augment: bool = True
    ):
        self.raw_dir = raw_dir
        self.mixed_dir = mixed_dir
        self.sample_rate = sample_rate
        self.segment_length = segment_length
        self.augment = augment
        
        # Trouver les paires
        self.pairs = self._find_pairs()
        print(f"Found {len(self.pairs)} vocal pairs")
    
    def _find_pairs(self) -> List[Tuple[str, str]]:
        """Trouve les paires raw/mixed"""
        pairs = []
        raw_files = {f for f in os.listdir(self.raw_dir) if f.endswith('.wav')}
        mixed_files = {f for f in os.listdir(self.mixed_dir) if f.endswith('.wav')}
        
        for filename in sorted(raw_files):
            if filename in mixed_files:
                pairs.append((
                    os.path.join(self.raw_dir, filename),
                    os.path.join(self.mixed_dir, filename)
                ))
        return pairs
    
    def _load_audio(self, path: str) -> np.ndarray:
        """Charge audio en mono"""
        audio, sr = librosa.load(path, sr=self.sample_rate, mono=True)
        return audio
    
    def _extract_segment(self, audio: np.ndarray) -> np.ndarray:
        """Extrait un segment de 4 secondes"""
        if len(audio) <= self.segment_length:
            padded = np.zeros(self.segment_length)
            padded[:len(audio)] = audio
            return padded
        else:
            # M√™me position pour raw et mixed (important!)
            max_start = len(audio) - self.segment_length
            start = random.randint(0, max_start) if self.augment else 0
            return audio[start:start + self.segment_length]
    
    def __len__(self):
        return len(self.pairs)
    
    def __getitem__(self, idx):
        raw_path, mixed_path = self.pairs[idx]
        
        # Charger
        raw = self._load_audio(raw_path)
        mixed = self._load_audio(mixed_path)
        
        # Extraire segments (m√™me position)
        if len(raw) > self.segment_length and len(mixed) > self.segment_length:
            max_start = min(len(raw), len(mixed)) - self.segment_length
            start = random.randint(0, max_start) if self.augment else 0
            raw_seg = raw[start:start + self.segment_length]
            mixed_seg = mixed[start:start + self.segment_length]
        else:
            raw_seg = self._extract_segment(raw)
            mixed_seg = self._extract_segment(mixed)
        
        # Convertir en tensor [batch, channels, samples]
        # Pour Diff-MST : [1, 1, samples] (1 track, 1 channel)
        raw_tensor = torch.FloatTensor(raw_seg).unsqueeze(0).unsqueeze(0)
        mixed_tensor = torch.FloatTensor(mixed_seg).unsqueeze(0).unsqueeze(0)
        
        return {
            'tracks': raw_tensor,  # Piste √† mixer
            'reference': mixed_tensor  # Reference mix (ce qu'on veut obtenir)
        }
```

### √âtape 3 : Cr√©er Script de Training Custom

**Cr√©er `train_vocal_diffmst.py` :**

```python
"""
Fine-tuning Diff-MST pour vocal mixing
"""
import torch
from pytorch_lightning import Trainer, LightningModule
from pytorch_lightning.callbacks import ModelCheckpoint
from torch.utils.data import DataLoader
import sys
sys.path.append('/content/Diff-MST')

from mst.modules import DiffMSTModel
from mst.dataloaders.vocal_reference_dataset import VocalReferenceDataset

class VocalMixingModule(LightningModule):
    """Module Lightning pour vocal mixing"""
    
    def __init__(self):
        super().__init__()
        # Charger Diff-MST pr√©-entra√Æn√© ou cr√©er nouveau
        self.model = DiffMSTModel(
            n_tracks=1,  # Single track
            sample_rate=48000
        )
    
    def training_step(self, batch, batch_idx):
        tracks = batch['tracks']  # Raw vocal
        reference = batch['reference']  # Mixed vocal (target)
        
        # Diff-MST pr√©dit les param√®tres et applique
        output = self.model(tracks, reference)
        
        # Loss : comparer output avec reference
        loss = torch.nn.functional.l1_loss(output, reference)
        
        self.log('train_loss', loss)
        return loss
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=1e-5)

# Dataset
dataset = VocalReferenceDataset(
    raw_dir="/content/dataset/raw",
    mixed_dir="/content/dataset/mixed",
    sample_rate=48000,
    segment_length=192000,
    augment=True
)

train_loader = DataLoader(
    dataset,
    batch_size=4,
    shuffle=True,
    num_workers=0
)

# Trainer
trainer = Trainer(
    max_epochs=100,
    gpus=1 if torch.cuda.is_available() else 0,
    callbacks=[
        ModelCheckpoint(
            dirpath="/content/models",
            filename="vocal_mixing-{epoch:02d}-{loss:.4f}",
            save_top_k=3,
            monitor="train_loss"
        )
    ]
)

# Training
model = VocalMixingModule()
trainer.fit(model, train_loader)
```

---

## ‚ö†Ô∏è Probl√®mes Potentiels et Solutions

### Probl√®me 1 : Diff-MST N√©cessite Plusieurs Pistes

**Solution :** Utiliser 1 piste (vocal) et adapter le mod√®le pour accepter n_tracks=1

### Probl√®me 2 : Reference Mix Format

**Solution :** Utiliser le mixed comme reference mix (m√™me format)

### Probl√®me 3 : Architecture Complexe

**Solution :** On explore le code Diff-MST ensemble et on adapte √©tape par √©tape

---

## üéØ Plan d'Action Concret

### Phase 1 : Explorer Diff-MST (Maintenant)

1. ‚úÖ Cloner le repo
2. ‚úÖ Lire la structure
3. ‚úÖ Identifier les fichiers cl√©s
4. ‚úÖ Comprendre comment adapter

### Phase 2 : Cr√©er DataLoader (1-2 heures)

1. ‚úÖ Cr√©er `VocalReferenceDataset`
2. ‚úÖ Tester avec quelques fichiers
3. ‚úÖ V√©rifier que √ßa fonctionne

### Phase 3 : Adapter le Mod√®le (2-4 heures)

1. ‚úÖ Modifier pour single track
2. ‚úÖ Adapter la loss function
3. ‚úÖ Tester le forward pass

### Phase 4 : Fine-Tuning (6-8 heures)

1. ‚úÖ Lancer le training
2. ‚úÖ Monitorer
3. ‚úÖ Sauvegarder checkpoints

---

## üí° Alternative : Commencer Simple

**Si c'est trop complexe au d√©but :**

1. **Essayer d'abord** avec notre mod√®le actuel + 222 paires
2. **Voir la qualit√©** obtenue
3. **Si pas satisfait** ‚Üí adapter Diff-MST

**Mais tu as raison :** Diff-MST = meilleure qualit√© √† long terme.

---

## üöÄ On Commence ?

**Je propose :**

1. **Explorer Diff-MST** (voir ce qu'il faut adapter)
2. **Cr√©er le DataLoader custom** ensemble
3. **Adapter le mod√®le** si n√©cessaire
4. **Lancer le fine-tuning**

**Tu veux qu'on commence par explorer le code Diff-MST pour voir exactement ce qu'il faut adapter ?**
